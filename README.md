# Image captioning using CNN, LSTM and Transfer learning
```
Domain             : Computer Vision, NLP, Machine Learning
Sub-Domain         : Deep Learning, Image Recognition, Sequences
Techniques         : Deep Convolutional Neural Network, Wordembedding (glove), Inception v3
Application        : Self driving cars, CCTV safety, aid to blind etc.
```
## Dataset
```
Dataset Name     : Flickr 8k dataset, Flickr 8k text
Dataset Link     : https://forms.illinois.edu/sec/1713398

```      
### Dataset Details
```
Dataset Name            : Flickr 8k,Flickr 8k text
Number of Class         : 1
Number/Size of Images   : Total      : 8091 
                          Training   : 6000  
                          Testing    : 2000

Model Parameters
Machine Learning Library: Keras
Optimizers              : Adam
Loss Function           : categorical_crossentropy

For Custom Deep Convolutional Neural Network : 
Training Parameters
Batch Size              : 64
Number of Epochs        : 20
Training Time           : about 150-180 minutes (using GPU)

Output 
Training set loss       : 2.5621
```

### Detailed results
```
check the im_cap.ipynb file to see the results in the last modules.
```
### Tools and Libraries
```
Languages               : Python
Tools/IDE               : Anaconda, Jupyter Notebook
Libraries               : Keras, numpy, pandas, matplotlib, glob, pickle, time (check the notebook for complete list)
```
### Check the major_report document for full documentation.
### This project is based on the works of Harshall Lamba (https://github.com/hlamba28/Automatic-Image-Captioning)
